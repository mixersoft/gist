using Nemerle.Collections;
using PipelineMacro;
using System.Collections.Generic;
using System.Drawing;
using System.IO;
using System;

[ Record ]
class FindDuplicatesTask : ITask
{
	public MapItems(items : list[ITaskItem]) : list[TaskMapping]
	{
		mutable mappings = [];
		foreach ((name, json, images) in items.Join.[JsonItem, ImageCollectionItem]())
			mappings ::= TaskMapping(this, [ json, images ], [ DuplicatesItem(name) ]);
		mappings;
	}

	private Compute
		( [InItem]  json       : JsonItem
		, [InItem]  images     : ImageCollectionItem
		, [OutItem] duplicates : DuplicatesItem
		, duplicateThreshold : int
		, duplicateHash      : ImageHash.HashMethod
		) : void
	{
		def HaveAllImages()
		{
			def imageIDs = Set(images.ImageIDs);
			json.Photos.ForAll(photo => imageIDs.Contains(photo.ID));
		}
		when (!HaveAllImages())
			throw InconsistentItemsException("Missing photo images.", [ json, images ]);


		def Hash(id)
		{
			using (bmp = Bitmap(images.MakePath(id)))
				ImageHash.GetMeanHashCodeLong(bmp, duplicateHash);
		}
		def HammingDistance(a : ulong, b : ulong) : int
		{
			mutable xor = a ^ b;
			mutable distance = 0ul;
			while (xor != 0)
			{
				distance += xor & 1ul;
				xor >>= 1;
			}
			distance :> int;
		}

		if (json.Photos.Length == 0)
		{
			duplicates.ImageIDs = array(0);
		}
		else
		{
			def groups = List(); // sequence of groups
			def group  = List(); // sequence of duplicates

			def ids = json.Photos.Sort(_.DateTaken).Map(_.ID);

			group.Add(ids[0]);

			mutable prev = Hash(ids[0]);

			for (mutable i = 1; i != ids.Length; ++i)
			{
				def curr = Hash(ids[i]);
				when (HammingDistance(curr, prev) > duplicateThreshold)
				{
					//start a new group with the current item
					groups.Add(group.ToArray());
					group.Clear();
				}
				group.Add(ids[i]);
				prev = curr;
			}
			groups.Add(group.ToArray());

			duplicates.ImageIDs = groups.ToArray();
		}
	}
}
